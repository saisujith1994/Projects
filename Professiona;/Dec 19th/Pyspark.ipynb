{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2acf6ebc-a436-419f-aefc-02f248f42802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Develop User-Defined Functions (UDFs) using Pandas/Python UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7530ee6-4c0a-44db-b1da-a8df72992930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"mynum\",\"10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52addc50-5df2-4c06-9af3-330923d11461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n=dbutils.widgets.get(\"mynum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04513a65-06a4-47b0-b22c-8b1963be5538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, lit\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "def operationplus(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    return a + b\n",
    "\n",
    "operationplus_udf = pandas_udf(\n",
    "    operationplus,\n",
    "    returnType=LongType()\n",
    ")\n",
    "\n",
    "def stringconcat(a: pd.Series, b: pd.Series)-> pd.Series:\n",
    "    return a + b\n",
    "\n",
    "stringconcat_udf = pandas_udf(\n",
    "    stringconcat,\n",
    "    returnType=StringType())\n",
    "display(df.withColumn(\"BossCategory\",stringconcat_udf(df.GameName,df.GameType)))\n",
    "x = pd.Series(range(10))\n",
    "print(operationplus(x, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e6acf6-d670-49a9-a908-b249c83cc7ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "myschema= StructType([StructField(\"Id\",IntegerType(),True),\\\n",
    "\n",
    "    StructField(\"GameName\",StringType(),True),\\\n",
    "        StructField(\"GameType\",StringType(),True),\\\n",
    "            StructField(\"GameDate\",StringType(),True)])\n",
    "data=[(1,\"Contra\",\"Action\",\"1987-01-01\"),(2,\"Super Mario\",\"Adventure\",\"1985-01-01\"),(3,\"PacMan\",\"Action\",\"1980-01-01\"),(4,\"Donkey Kong\",\"Action\",\"1981-01-01\"),(5,\"Galaga\",\"Action\",\"1981-01-01\"),(6,\"Space Invaders\",\"Action\",\"1978-01-01\"),(7,\"Tetris\",\"Puzzle\",\"1984-01-01\"),(8,\"Ms PacMan\",\"Action\",\"1981-01-01\"),(9,\"Asteroids\",\"Action\",\"1979-01-01\"),(10,\"Space Invaders\",\"Action\",\"1978-01-01\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8036e027-97a3-4462-aa7e-1a483aff341d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df= spark.createDataFrame(data,myschema)\n",
    "df.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83993de9-1738-449c-bc11-7355f9796d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Build and manage reliable, production-ready data pipelines for batch and\n",
    "streaming data using Lakeflow Spark Declarative Pipelines and Autoloader.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbaa13d3-0590-426c-9fec-ec28fc5f1c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create and Automate ETL workloads using Jobs via UI/APIs/CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81c5864b-2ff6-426a-b4fd-c49867368bb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Use APPLY CHANGES APIs to simplify CDC in Lakeflow Spark Declarative\n",
    "Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93939893-c31b-49ff-899f-1a797d95f82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a pipeline component that uses control flow operators (e.g., if/else,\n",
    "for/each, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db0441ba-27af-47f6-8c3d-286ee0a50289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Develop unit and integration tests using assertDataFrameEqual,\n",
    "assertSchemaEqual, DataFrame.transform, and testing frameworks, to ensure\n",
    "code correctness, including a built-in debugger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e4693e9-5991-4e6d-8ea2-ebaf0c7a5346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Design and implement data ingestion pipelines to efficiently ingest a variety of data\n",
    "formats including Delta Lake, Parquet, ORC, AVRO, JSON, CSV, XML, Text and Binary from\n",
    "diverse sources such as message buses and cloud storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1886247-2f12-40e8-ad00-6dc8b2f381ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create an append-only data pipeline capable of handling both batch and streaming\n",
    "data using Delta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "188dac41-4692-4d87-a9ca-806f7291e9df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write efficient Spark SQL and PySpark code to apply advanced data transformations,\n",
    "including window functions, joins, and aggregations, to manipulate and analyze large\n",
    "Datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a81f381-39d7-4e75-995b-047e751934d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Develop a quarantining process for bad data with Lakeflow Spark Declarative Pipelines,\n",
    "or autoloader in classic jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2acb41e9-4142-41f6-8b8c-6ed0ba540e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Use system tables for observability over resource utilization, cost, auditing and workload monitoring.\n",
    "- Use Query Profiler UI and Spark UI to monitor workloads.\n",
    "- Use the Databricks REST APIs/Databricks CLI for monitoring jobs and pipelines.\n",
    "- Use Lakeflow Spark Declarative Pipelines Event Logs to monitor pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58747b1b-02c5-478a-9ad1-847bee3dbc70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark",
   "widgets": {
    "mynum": {
     "currentValue": "20",
     "nuid": "8f112b53-f720-429d-953f-ff7c7f69f860",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "10",
      "label": null,
      "name": "mynum",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "10",
      "label": null,
      "name": "mynum",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
